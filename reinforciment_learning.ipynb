{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import io\n",
    "from data_generator import DataGenerator\n",
    "from trade_env import TraderEnv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_length = 10\n",
    "\n",
    "def get_enviroment(stage_length=10):\n",
    "    return TraderEnv(DataGenerator(), stage_history_length=stage_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.        ,  1.00091519,  1.00091589,  1.00097681,  1.00114556,\n",
       "        1.00123519,  1.00131782,  1.00131852,  1.00180868,  1.00185209,\n",
       "        1.00204675,  1.00236885,  1.00255791,  1.00255861,  1.00256772,\n",
       "        1.00257402,  1.00260133,  1.00275608,  1.00320912,  1.00326514,\n",
       "       -0.99766616, -0.99766546, -0.99634414, -0.99634204, -0.99585888,\n",
       "       -0.99513485, -0.99506903, -0.99506833, -0.99382404, -0.99382334,\n",
       "       -0.99382123, -0.99254053, -0.9915154 , -0.99081518, -0.99011496,\n",
       "       -0.98983487, -0.98941473, -0.98940143, -0.98871451, -0.98801429,\n",
       "        1.        ,  0.        ,  0.        ,  0.        ,  0.        ])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stage_length_var = 1\n",
    "trade = get_enviroment(stage_length_var)\n",
    "trade.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.1   {'status': 'reward too low -500.1000000000452'}\n"
     ]
    }
   ],
   "source": [
    "for data_index in range(50000):\n",
    "    next_state, reward, done, info  = trade.step(0)\n",
    "    if done:\n",
    "        print(reward, \" \", info)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samir/miniconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pylab\n",
    "import time\n",
    "import gym\n",
    "from keras.layers import Dense, Input\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "# global variables for threading\n",
    "episode = 0\n",
    "scores = []\n",
    "\n",
    "EPISODES = 30000\n",
    "\n",
    "# This is A3C(Asynchronous Advantage Actor Critic) agent(global) for the Cartpole\n",
    "# In this example, we use A3C algorithm\n",
    "class A3CAgent:\n",
    "    def __init__(self, state_size, action_size, env_name):\n",
    "        # get size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        episode = 0\n",
    "        scores = []\n",
    "\n",
    "        # get gym environment name\n",
    "        self.env_name = env_name\n",
    "\n",
    "        # these are hyper parameters for the A3C\n",
    "        self.actor_lr = 0.00002\n",
    "        self.critic_lr = 0.00002\n",
    "        self.discount_factor = .6\n",
    "\n",
    "        self.threads = 32\n",
    "\n",
    "        # create model for actor and critic network\n",
    "        self.actor, self.critic = self.build_model()\n",
    "\n",
    "        # method for training actor and critic network\n",
    "        self.optimizer = [self.actor_optimizer(), self.critic_optimizer()]\n",
    "\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        K.set_session(self.sess)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # approximate policy and value using Neural Network\n",
    "    # actor -> state is input and probability of each action is output of network\n",
    "    # critic -> state is input and value of state is output of network\n",
    "    # actor and critic network share first hidden layer\n",
    "    def build_model(self):\n",
    "        state = Input(batch_shape=(None,  self.state_size))\n",
    "        shared = Dense(self.state_size*2, input_dim=self.state_size, activation='relu', kernel_initializer='glorot_uniform')(state)\n",
    "        \n",
    "        actor_hidden = Dense(self.state_size, activation='relu', kernel_initializer='glorot_uniform')(shared)\n",
    "        actor_hidden = Dense(128, activation='relu', kernel_initializer='glorot_uniform')(actor_hidden)\n",
    "        actor_hidden = Dense(64, activation='relu', kernel_initializer='glorot_uniform')(actor_hidden)\n",
    "        actor_hidden = Dense(32, activation='relu', kernel_initializer='glorot_uniform')(actor_hidden)\n",
    "        actor_hidden = Dense(16, activation='relu', kernel_initializer='glorot_uniform')(actor_hidden)\n",
    "        action_prob = Dense(self.action_size, activation='softmax', kernel_initializer='glorot_uniform')(actor_hidden)\n",
    "\n",
    "        value_hidden = Dense(self.state_size, activation='relu', kernel_initializer='he_uniform')(shared)\n",
    "        value_hidden = Dense(128, activation='relu', kernel_initializer='he_uniform')(value_hidden)\n",
    "        value_hidden = Dense(64, activation='relu', kernel_initializer='he_uniform')(value_hidden)\n",
    "        value_hidden = Dense(32, activation='relu', kernel_initializer='he_uniform')(value_hidden)\n",
    "        value_hidden = Dense(16, activation='relu', kernel_initializer='he_uniform')(value_hidden)\n",
    "        state_value = Dense(1, activation='linear', kernel_initializer='he_uniform')(value_hidden)\n",
    "\n",
    "        actor = Model(inputs=state, outputs=action_prob)\n",
    "        critic = Model(inputs=state, outputs=state_value)\n",
    "\n",
    "        actor._make_predict_function()\n",
    "        critic._make_predict_function()\n",
    "\n",
    "        #actor.summary()\n",
    "        #critic.summary()\n",
    "\n",
    "        return actor, critic\n",
    "\n",
    "    # make loss function for Policy Gradient\n",
    "    # [log(action probability) * advantages] will be input for the back prop\n",
    "    # we add entropy of action probability to loss\n",
    "    def actor_optimizer(self):\n",
    "        action = K.placeholder(shape=(None, self.action_size))\n",
    "        advantages = K.placeholder(shape=(None, ))\n",
    "\n",
    "        policy = self.actor.output\n",
    "\n",
    "        good_prob = K.sum(action * policy, axis=1)\n",
    "        eligibility = K.log(good_prob + 1e-10) * K.stop_gradient(advantages)\n",
    "        loss = -K.sum(eligibility)\n",
    "\n",
    "        entropy = K.sum(policy * K.log(policy + 1e-10), axis=1)\n",
    "\n",
    "        actor_loss = loss + 0.01*entropy\n",
    "\n",
    "        optimizer = Adam(lr=self.actor_lr)\n",
    "        updates = optimizer.get_updates(self.actor.trainable_weights, [], actor_loss)\n",
    "        train = K.function([self.actor.input, action, advantages], [], updates=updates)\n",
    "        return train\n",
    "\n",
    "    # make loss function for Value approximation\n",
    "    def critic_optimizer(self):\n",
    "        discounted_reward = K.placeholder(shape=(None, ))\n",
    "\n",
    "        value = self.critic.output\n",
    "\n",
    "        loss = K.mean(K.square(discounted_reward - value))\n",
    "\n",
    "        optimizer = Adam(lr=self.critic_lr)\n",
    "        updates = optimizer.get_updates(self.critic.trainable_weights, [], loss)\n",
    "        train = K.function([self.critic.input, discounted_reward], [], updates=updates)\n",
    "        return train\n",
    "\n",
    "    # make agents(local) and start training\n",
    "    def train(self):\n",
    "        # self.load_model('./save_model/cartpole_a3c.h5')\n",
    "        agents = [Agent(i, self.actor, self.critic, self.optimizer, self.env_name, self.discount_factor,\n",
    "                        self.action_size, self.state_size) for i in range(self.threads)]\n",
    "\n",
    "        for agent in agents:\n",
    "            agent.start()\n",
    "\n",
    "        for agent in agents:\n",
    "            agent.join()\n",
    "\n",
    "        plot = scores[:]\n",
    "        pylab.plot(range(len(plot)), plot, 'b')\n",
    "        pylab.savefig(\"./save_graph/model.png\")\n",
    "\n",
    "        self.save_model('./save_model/model')\n",
    "        \n",
    "        \n",
    "    def save_model(self, name):\n",
    "        self.actor.save_weights(name + \"_actor.h5\")\n",
    "        self.critic.save_weights(name + \"_critic.h5\")\n",
    "\n",
    "    def load_model(self, name):\n",
    "        self.actor.load_weights(name + \"_actor.h5\")\n",
    "        self.critic.load_weights(name + \"_critic.h5\")\n",
    "\n",
    "# This is Agent(local) class for threading\n",
    "class Agent(threading.Thread):\n",
    "    def __init__(self, index, actor, critic, optimizer, env_name, discount_factor, action_size, state_size):\n",
    "        threading.Thread.__init__(self)\n",
    "\n",
    "        self.states = []\n",
    "        self.rewards = []\n",
    "        self.actions = []\n",
    "\n",
    "        self.index = index\n",
    "        self.actor = actor\n",
    "        self.critic = critic\n",
    "        self.optimizer = optimizer\n",
    "        self.env_name = env_name\n",
    "        self.discount_factor = discount_factor\n",
    "        self.action_size = action_size\n",
    "        self.state_size = state_size\n",
    "\n",
    "    # Thread interactive with environment\n",
    "    def run(self):\n",
    "        global episode\n",
    "        env = get_enviroment(stage_length_var)\n",
    "        while episode < EPISODES:\n",
    "            state = env.reset()\n",
    "            score = 0\n",
    "            \n",
    "            while True:\n",
    "                action = self.get_action(state)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                score += reward\n",
    "\n",
    "                self.memory(state, action, reward)\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "                if done or score < env.minimum_reward_limit:\n",
    "                    episode += 1\n",
    "                    #print(\"episode: \", episode, \"/ score : \", score)\n",
    "                    scores.append(score)\n",
    "                    self.train_episode(score < 0)\n",
    "                    break\n",
    "        \n",
    "    # In Policy Gradient, Q function is not available.\n",
    "    # Instead agent uses sample returns for evaluating policy\n",
    "    def discount_rewards(self, rewards, done=True):\n",
    "        discounted_rewards = np.zeros_like(rewards)\n",
    "        running_add = 0\n",
    "        if not done:\n",
    "            running_add = self.critic.predict(np.reshape(self.states[-1], (1, self.state_size)))[0]\n",
    "        for t in reversed(range(0, len(rewards))):\n",
    "            running_add = running_add * self.discount_factor + rewards[t]\n",
    "            discounted_rewards[t] = running_add\n",
    "        return discounted_rewards\n",
    "\n",
    "    # save <s, a ,r> of each step\n",
    "    # this is used for calculating discounted rewards\n",
    "    def memory(self, state, action, reward):\n",
    "        self.states.append(state)\n",
    "        act = np.zeros(self.action_size)\n",
    "        act[action] = 1\n",
    "        self.actions.append(act)\n",
    "        self.rewards.append(reward)\n",
    "\n",
    "    # update policy network and value network every episode\n",
    "    def train_episode(self, done):\n",
    "        discounted_rewards = self.discount_rewards(self.rewards, done)\n",
    "\n",
    "        values = self.critic.predict(np.array(self.states))\n",
    "        values = np.reshape(values, len(values))\n",
    "\n",
    "        advantages = discounted_rewards - values\n",
    "\n",
    "        self.optimizer[0]([self.states, self.actions, advantages])\n",
    "        self.optimizer[1]([self.states, discounted_rewards])\n",
    "        self.states, self.actions, self.rewards = [], [], []\n",
    "\n",
    "    def get_action(self, state):\n",
    "        policy = self.actor.predict(np.reshape(state, [1, self.state_size]))[0]\n",
    "        return np.random.choice(self.action_size, 1, p=policy)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#############################\n",
      "discount_factor 0.800000 stage_history_length 30 \n",
      "#############################\n"
     ]
    }
   ],
   "source": [
    "\n",
    "discount_factor = 0\n",
    "stage_history_length = 0\n",
    "\n",
    "discount_factor_start = 0.1\n",
    "stage_history_length_start = 10\n",
    "\n",
    "actor_lr_max, critic_lr_max, discount_factor_max, stage_history_length_max = 0, 0, 0, 0\n",
    "\n",
    "max_score = 0\n",
    "\n",
    "max_config = \"\"\n",
    "\n",
    "is_first = True\n",
    "\n",
    "global_agent = A3CAgent(1, 2, \"TraderEnv\")\n",
    "\n",
    "for discount_factor_x in range(3, 9):\n",
    "    discount_factor = round(discount_factor_start + (discount_factor_x * discount_factor_start), 2)\n",
    "\n",
    "    for stage_history_length_x in range(1, 4):\n",
    "        stage_history_length = int(stage_history_length_start + (stage_history_length_x * stage_history_length_start))\n",
    "        \n",
    "        stage_length_var=stage_history_length\n",
    "\n",
    "        env = TraderEnv(DataGenerator(), stage_history_length=stage_history_length)\n",
    "\n",
    "        state_size = env.observation_space.shape[0]\n",
    "        action_size = env.action_space.n\n",
    "\n",
    "        global_agent = A3CAgent(state_size, action_size, \"TraderEnv\")\n",
    "        \n",
    "        global_agent.discount_factor = discount_factor\n",
    "        \n",
    "        config = \"discount_factor %f stage_history_length %s \" % (global_agent.discount_factor, stage_history_length)\n",
    "\n",
    "        print(\"#############################\")\n",
    "        print(config)\n",
    "        print(\"#############################\")\n",
    "        \n",
    "        global_agent.train()\n",
    "        \n",
    "        print(\"#############################\")\n",
    "        print(\"Trainning finished\")\n",
    "        print(\"#############################\")\n",
    "        \n",
    "        def get_trained_score(data_index, stage_history_length):\n",
    "\n",
    "            env = TraderEnv(DataGenerator(random=False, first_index=data_index), stage_history_length=stage_history_length)\n",
    "\n",
    "            def get_action(state):\n",
    "                policy =  global_agent.actor.predict(np.reshape(state, [1, state_size]))[0]\n",
    "                return np.argmax(policy)\n",
    "\n",
    "            state = env.reset()\n",
    "            score = 0\n",
    "            while True:\n",
    "                action = get_action(state)\n",
    "                #print(action)\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                score += reward\n",
    "\n",
    "                state = next_state\n",
    "                if done:\n",
    "                    print (\"score: \", score , \"info\", info)\n",
    "                    break\n",
    "            return score\n",
    "        \n",
    "        score = 0\n",
    "              \n",
    "        for data_index in range(1, 3):\n",
    "            score += get_trained_score(1000*data_index, stage_history_length)\n",
    "        \n",
    "        print(\"#############################\")\n",
    "        print(\"%s: %s \" % (config, score))\n",
    "        print(\"#############################\")\n",
    "        if score > max_score or is_first:\n",
    "            is_first = False\n",
    "            max_score = score\n",
    "            max_config = config\n",
    "            discount_factor_max, stage_history_length_max = discount_factor, stage_history_length\n",
    "            print(\"Record: %s: %s \" % (config, score))\n",
    "            global_agent.save_model('./save_model/best')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (max_config, \" :: \", max_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TraderEnv(DataGenerator(), stage_history_length=30)\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size, action_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_agent = A3CAgent(state_size, action_size, \"TraderEnv\")\n",
    "global_agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(state):\n",
    "    policy =  global_agent.actor.predict(np.reshape(state, [1, state_size]))[0]\n",
    "    return np.argmax(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_factor = 0.900000 \n",
    "stage_history_length = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = TraderEnv(DataGenerator(random=False, first_index=data_index), stage_history_length=stage_history_length)\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "global_agent = A3CAgent(state_size, action_size, \"TraderEnv\")\n",
    "\n",
    "global_agent.discount_factor = discount_factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_agent.load_model('./save_model/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = 0\n",
    "def get_trained_score(data_index, stage_history_length):\n",
    "    print (data_index)\n",
    "    env = TraderEnv(DataGenerator(random=False, first_index=data_index), stage_history_length=stage_history_length)\n",
    "\n",
    "    def get_action(state):\n",
    "        policy =  global_agent.actor.predict(np.reshape(state, [1, state_size]))[0]\n",
    "        return np.argmax(policy)\n",
    "\n",
    "    state = env.reset()\n",
    "    score = 0\n",
    "    while True:\n",
    "        action = get_action(state)\n",
    "        #print(action)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print (\"score: \", score , \"info\", info)\n",
    "            break\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_index in range(1, 3):\n",
    "    score += get_trained_score(100*data_index, stage_history_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(0.30000000000000004, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
